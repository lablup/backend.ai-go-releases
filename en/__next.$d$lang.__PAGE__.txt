1:"$Sreact.fragment"
2:I[17467,["/backend.ai-go-releases/_next/static/chunks/eac606b0e22439af.js"],"LanguageSwitcher"]
3:I[85161,["/backend.ai-go-releases/_next/static/chunks/eac606b0e22439af.js"],"Hero"]
4:I[96,["/backend.ai-go-releases/_next/static/chunks/eac606b0e22439af.js"],"Features"]
5:I[50342,["/backend.ai-go-releases/_next/static/chunks/eac606b0e22439af.js"],"Downloads"]
6:I[2718,["/backend.ai-go-releases/_next/static/chunks/a66e09a7c2336f67.js"],"OutletBoundary"]
7:"$Sreact.suspense"
0:{"buildId":"yVxc2IZMs64kBh_ia8-PP","rsc":["$","$1","c",{"children":[["$","main",null,{"className":"min-h-screen bg-bg-void selection:bg-brand-cyan/30","children":[["$","$L2",null,{}],["$","$L3",null,{"dict":{"badge":"Backend.AI GO","title_prefix":"Your AI,","title_suffix":"Your Machine","description":"Unleash the full potential of your hardware, from MacBooks to DGX Spark.\nRun local inference, manage agents, and cluster resources seamlessly.\nOne app, infinite possibilities.","download_btn":"Download for","download_latest":"Download Latest","release_notes":"Release Notes","other_platforms":"Other Platforms","checking":"Checking Release...","view_releases":"View Releases"}}],["$","$L4",null,{"dict":{"title":"Complete AI Orchestration","subtitle":"From single-device inference to multi-node clustering, Backend.AI GO scales with your needs.","items":[{"title":"Local Inference Engine","description":"Run llama.cpp, mlx-lm, and stable-diffusion.cpp natively on Windows, macOS, Linux, and personal workstations like DGX Spark. Hardware acceleration for CUDA, ROCm, Metal, and Intel Arc."},{"title":"Personal AI Agents","description":"Built-in Chat, Image Generation, and Agent capabilities with Tool Calling and MCP (Model Context Protocol) for secure personal tasks."},{"title":"P2P Clustering","description":"Connect up to 32 Backend.AI GO instances to create a personal compute cluster, or join enterprise Backend.AI clusters."},{"title":"Cloud & Remote Model Integration","description":"Seamlessly integrate OpenAI, Anthropic, Gemini, or connect to remote vLLM/Ollama servers as if they were local."},{"title":"Security & Privacy","description":"Your data stays with you. Run sensitive workloads locally without data leaving your infrastructure."},{"title":"Service Router","description":"Expose all your local, cluster, and cloud AI resources through a single, unified API endpoint."}]}}],["$","$L5",null,{"dict":{"title":"All Downloads","subtitle":"Specific binaries for your architecture. Version","view_full":"View full release notes on GitHub"}}],["$","footer",null,{"className":"py-12 border-t border-white/5 bg-black/40 backdrop-blur-md","children":["$","div",null,{"className":"container mx-auto px-4 flex flex-col md:flex-row justify-between items-center gap-6","children":[["$","div",null,{"className":"text-gray-500 text-sm","children":["Â© ",2026," ","Lablup Inc. All rights reserved."]}],["$","div",null,{"className":"flex gap-6 text-sm text-gray-500","children":[["$","a",null,{"href":"https://backend.ai","target":"_blank","className":"hover:text-white transition-colors","children":"Backend.AI"}],["$","a",null,{"href":"https://github.com/lablup/backend.ai-go-releases","target":"_blank","className":"hover:text-white transition-colors","children":"GitHub"}],["$","a",null,{"href":"#","className":"hover:text-white transition-colors","children":"Docs"}]]}]]}]}]]}],[["$","script","script-0",{"src":"/backend.ai-go-releases/_next/static/chunks/eac606b0e22439af.js","async":true}]],["$","$L6",null,{"children":["$","$7",null,{"name":"Next.MetadataOutlet","children":"$@8"}]}]]}],"loading":null,"isPartial":false}
8:null
